{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "buildmodel_tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMI63xhcX70imbfZwI0ABeE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monta0315/pytorch_pra/blob/main/buildmodel_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp5P2ZCWjM_y"
      },
      "source": [
        "#ニューラルネットワークはレイヤー（モジュール）の塊で構成\n",
        "tourch.nnで構築"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaY4iU_kjGYd"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDqU7okCjy3E"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import  datasets,transforms"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ6d9LjKkCge",
        "outputId": "f9575948-cbfa-4673-c142-00479d7699a3"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7KD9--YkfPF"
      },
      "source": [
        "#クラスの定義\n",
        "nn.moduleを継承し、独自のニューラルネットワークを定義し、その後のネットワークのレイヤーを__init__で初期化する\n",
        "nn.moduleを継承した全てのモジュールは入力データである順伝播関数であるforword関数を持つ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKA35-JCkWW0"
      },
      "source": [
        "from torch.nn.modules import linear\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork,self).__init__()\n",
        "        self.flatten =  nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV1uk4kLmnGm",
        "outputId": "86b1402a-9f65-464c-e080-95b82dcac2a2"
      },
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuKSz2rsnITl"
      },
      "source": [
        "モデルを利用するためには、入力データを与える必要がある\n",
        "入力データをモデルに投入するとforward関数で処理するとともにいくつかのbackgroundoperationsが実行される\n",
        "そのため、model.forward()と処理しては行けない"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykjh7usmoBus"
      },
      "source": [
        "今回のモデルにデータを与えると各クラスの生の予測値を含む10次元のテンソルを返す\n",
        "nn.softmaxモジュールにこの出力結果を与えることで、入力データが各クラスに属する確率の予測値を求めることができる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5-S0Xq4msTx",
        "outputId": "126c036c-6a7b-4d94-815f-7aae6661c7e9"
      },
      "source": [
        "X = torch.rand(1,28,28,device=device)\n",
        "print(X)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[2.8547e-02, 8.8291e-01, 8.5527e-01, 9.7479e-01, 4.2234e-01,\n",
            "          2.5545e-02, 8.8373e-01, 8.8922e-01, 4.6560e-01, 5.8928e-01,\n",
            "          7.1818e-01, 7.2105e-01, 3.8250e-01, 9.3117e-01, 3.4347e-02,\n",
            "          2.9115e-01, 5.3580e-01, 6.5166e-02, 1.5417e-01, 8.3660e-01,\n",
            "          8.1849e-01, 3.4793e-01, 6.1489e-01, 2.8478e-01, 8.2009e-01,\n",
            "          3.0203e-01, 9.7630e-01, 2.8259e-01],\n",
            "         [7.2598e-01, 4.6867e-01, 9.4671e-01, 4.9512e-01, 4.6459e-01,\n",
            "          1.6260e-01, 2.7993e-01, 2.2339e-01, 8.1853e-01, 8.9008e-01,\n",
            "          5.1195e-01, 5.1997e-01, 9.1786e-01, 7.1968e-03, 5.2666e-01,\n",
            "          4.9971e-01, 7.8524e-03, 1.7741e-01, 3.2455e-01, 5.9594e-01,\n",
            "          3.3010e-01, 5.3682e-01, 5.7743e-01, 1.8115e-01, 3.3265e-01,\n",
            "          5.4226e-01, 1.7629e-01, 8.2944e-01],\n",
            "         [4.9466e-01, 9.6260e-01, 7.8886e-01, 3.1613e-02, 3.1050e-01,\n",
            "          1.8888e-01, 2.2742e-01, 7.1561e-01, 8.2800e-01, 8.0766e-01,\n",
            "          7.4444e-01, 8.9336e-01, 6.4822e-01, 2.3441e-01, 5.8614e-01,\n",
            "          8.3081e-01, 4.6270e-01, 4.2995e-01, 6.0826e-01, 9.6725e-01,\n",
            "          7.5168e-01, 9.0565e-01, 5.2726e-01, 7.3136e-01, 6.2488e-01,\n",
            "          5.6326e-01, 2.2963e-01, 4.2857e-01],\n",
            "         [5.6838e-01, 5.3213e-01, 8.9611e-01, 1.8294e-01, 5.1814e-02,\n",
            "          6.9184e-01, 3.6394e-01, 2.3730e-03, 6.5817e-01, 2.4416e-01,\n",
            "          2.1894e-02, 5.4410e-01, 9.4424e-01, 5.7450e-01, 2.1104e-01,\n",
            "          8.8659e-03, 6.4090e-01, 4.5679e-01, 1.6038e-01, 4.8734e-01,\n",
            "          2.4025e-01, 2.3391e-01, 4.7722e-01, 5.9331e-01, 7.0693e-01,\n",
            "          3.5585e-01, 9.3254e-01, 8.7840e-01],\n",
            "         [3.4626e-01, 9.3185e-01, 1.3103e-02, 9.6881e-01, 7.4401e-01,\n",
            "          6.9168e-01, 8.7509e-01, 1.2660e-01, 3.2623e-01, 2.4872e-01,\n",
            "          4.9446e-01, 2.2150e-01, 1.8693e-01, 9.5476e-01, 7.4849e-01,\n",
            "          4.0396e-01, 7.5780e-01, 8.0738e-01, 2.2202e-01, 6.3291e-01,\n",
            "          2.4628e-02, 8.9134e-01, 7.7987e-01, 5.7564e-01, 5.1236e-01,\n",
            "          3.6258e-01, 3.1069e-01, 7.5693e-01],\n",
            "         [9.9697e-01, 7.7206e-01, 9.1170e-01, 6.8247e-02, 2.1898e-01,\n",
            "          7.1705e-02, 8.7246e-01, 5.7224e-01, 1.4148e-01, 4.7084e-01,\n",
            "          7.5508e-01, 6.5031e-01, 9.9654e-01, 4.4466e-01, 9.7206e-01,\n",
            "          4.8513e-01, 8.8598e-01, 4.0914e-01, 3.2365e-01, 6.1258e-01,\n",
            "          6.5394e-01, 6.0354e-01, 6.9520e-01, 9.5350e-01, 9.3391e-01,\n",
            "          8.2027e-01, 9.0551e-01, 7.2589e-01],\n",
            "         [9.5355e-01, 8.9367e-01, 2.8912e-01, 7.2383e-01, 7.5770e-01,\n",
            "          8.2964e-01, 3.2502e-01, 4.8059e-01, 5.2075e-01, 4.5999e-01,\n",
            "          1.1232e-01, 4.2745e-01, 1.7762e-01, 7.2445e-02, 4.5744e-01,\n",
            "          7.3899e-01, 7.6321e-02, 5.7282e-01, 5.1558e-01, 5.8312e-02,\n",
            "          5.4937e-01, 4.5095e-01, 4.2226e-01, 4.3570e-01, 5.8226e-01,\n",
            "          6.5259e-01, 9.4553e-01, 3.5858e-01],\n",
            "         [2.5856e-01, 2.2277e-01, 2.3004e-01, 7.1084e-01, 9.3148e-01,\n",
            "          9.8930e-01, 4.0714e-02, 6.5078e-01, 6.3538e-01, 9.7557e-01,\n",
            "          1.1753e-01, 5.4872e-01, 5.4630e-01, 2.1419e-01, 8.0669e-01,\n",
            "          3.7935e-01, 2.0742e-01, 6.8494e-01, 9.5258e-01, 2.0812e-01,\n",
            "          5.0880e-01, 8.9823e-01, 7.3258e-01, 7.8655e-01, 3.4347e-01,\n",
            "          7.9832e-01, 7.3469e-01, 1.7267e-01],\n",
            "         [7.9756e-01, 9.8551e-01, 1.4868e-01, 5.4963e-01, 6.1403e-01,\n",
            "          5.5286e-01, 1.1416e-01, 4.9134e-04, 4.1346e-02, 3.8811e-02,\n",
            "          7.0026e-01, 2.4083e-01, 7.6999e-01, 6.1989e-01, 4.5680e-01,\n",
            "          9.8519e-01, 4.8056e-01, 8.4961e-01, 7.1536e-01, 4.5101e-01,\n",
            "          2.4055e-01, 3.4635e-01, 8.0806e-01, 5.2195e-01, 9.5241e-01,\n",
            "          8.3278e-02, 4.2474e-01, 8.8467e-01],\n",
            "         [3.7234e-01, 4.5031e-01, 6.0517e-01, 9.5069e-01, 5.6480e-01,\n",
            "          1.6692e-01, 3.6522e-01, 7.8463e-02, 2.0349e-01, 7.8863e-01,\n",
            "          8.3330e-01, 2.5903e-01, 7.4076e-01, 9.6262e-01, 6.8246e-01,\n",
            "          9.1731e-01, 6.0423e-01, 9.0045e-01, 6.4723e-01, 7.3278e-02,\n",
            "          6.3271e-01, 9.2260e-01, 9.8568e-01, 1.4730e-01, 2.1537e-01,\n",
            "          5.6366e-01, 6.3205e-01, 4.0070e-01],\n",
            "         [8.4732e-02, 7.5405e-01, 5.8813e-01, 6.6690e-01, 8.8347e-01,\n",
            "          4.3794e-01, 7.3877e-01, 5.3124e-01, 5.0237e-02, 3.2302e-01,\n",
            "          3.8322e-01, 4.9481e-01, 4.2658e-01, 7.2945e-01, 6.2942e-01,\n",
            "          6.1264e-01, 3.4486e-01, 5.7308e-01, 8.5388e-01, 9.9383e-01,\n",
            "          6.9656e-01, 9.6029e-01, 9.4793e-01, 8.0049e-02, 7.6423e-01,\n",
            "          7.7975e-01, 1.5065e-01, 1.2381e-01],\n",
            "         [7.8083e-01, 8.9009e-01, 6.2739e-01, 6.5708e-03, 5.1036e-01,\n",
            "          7.3240e-01, 3.6697e-02, 7.5723e-02, 2.9587e-01, 2.5886e-01,\n",
            "          1.0237e-01, 7.4624e-02, 6.3902e-01, 8.5778e-01, 5.7480e-01,\n",
            "          9.6308e-01, 7.4697e-01, 5.9340e-01, 3.1458e-01, 6.1526e-01,\n",
            "          8.3725e-01, 7.2039e-01, 6.2794e-01, 1.2466e-01, 4.1756e-01,\n",
            "          1.6916e-01, 7.1434e-01, 2.0251e-01],\n",
            "         [9.3658e-01, 5.3473e-01, 3.2016e-01, 3.5218e-01, 3.9454e-01,\n",
            "          5.3372e-01, 5.5617e-01, 6.1572e-01, 6.9388e-01, 6.8485e-01,\n",
            "          7.9681e-01, 2.6787e-01, 9.8333e-01, 2.5190e-01, 2.1973e-01,\n",
            "          5.1444e-02, 9.5274e-01, 1.4658e-02, 2.8273e-01, 7.6824e-01,\n",
            "          3.3113e-01, 3.6551e-01, 5.2333e-01, 5.9499e-01, 9.6363e-01,\n",
            "          6.7163e-01, 4.3396e-01, 1.7269e-01],\n",
            "         [1.0928e-01, 6.6720e-01, 3.4533e-01, 6.2205e-01, 4.7980e-01,\n",
            "          1.4141e-01, 9.0463e-01, 9.9078e-01, 7.3292e-01, 5.8535e-01,\n",
            "          1.5152e-01, 9.9020e-01, 3.7155e-01, 4.1657e-02, 3.3326e-01,\n",
            "          3.1850e-01, 3.8620e-01, 9.9860e-01, 4.8167e-01, 2.5851e-01,\n",
            "          1.5981e-01, 6.9332e-01, 9.1559e-01, 9.5134e-01, 7.5144e-01,\n",
            "          2.5443e-01, 6.6263e-01, 9.4374e-01],\n",
            "         [3.2518e-01, 8.1264e-01, 1.4161e-01, 5.8274e-02, 1.9970e-01,\n",
            "          2.6678e-01, 9.2040e-01, 8.3270e-01, 3.6571e-01, 6.8245e-01,\n",
            "          1.9818e-01, 3.9128e-01, 2.7674e-01, 8.5574e-01, 4.0379e-02,\n",
            "          9.7274e-01, 9.0799e-01, 1.1039e-01, 7.6739e-01, 7.2439e-02,\n",
            "          5.1144e-01, 9.2599e-01, 7.8482e-01, 5.7351e-01, 1.0887e-01,\n",
            "          7.3405e-01, 5.2655e-01, 2.0987e-01],\n",
            "         [8.3809e-01, 6.5178e-01, 5.3646e-01, 9.7316e-01, 3.8644e-02,\n",
            "          9.4976e-01, 2.7567e-01, 6.0949e-01, 3.4032e-01, 5.2476e-01,\n",
            "          8.4113e-01, 3.2518e-02, 7.2280e-01, 6.7757e-01, 4.5831e-02,\n",
            "          8.0736e-01, 3.1468e-01, 7.3404e-01, 7.9420e-01, 2.0382e-01,\n",
            "          9.3547e-01, 9.6601e-01, 6.0193e-01, 8.7305e-01, 1.3561e-01,\n",
            "          2.0583e-01, 3.8255e-02, 8.8489e-01],\n",
            "         [8.0155e-01, 3.3615e-01, 3.4565e-01, 7.7237e-01, 8.6385e-01,\n",
            "          3.5166e-01, 9.4479e-01, 9.3789e-01, 5.3153e-01, 9.3122e-01,\n",
            "          4.2104e-02, 5.1731e-01, 9.2530e-02, 9.1320e-01, 9.8242e-01,\n",
            "          1.6371e-01, 1.2078e-01, 4.3370e-01, 2.1525e-02, 1.1000e-01,\n",
            "          2.0998e-01, 4.0767e-01, 3.7867e-01, 4.6470e-01, 6.0694e-01,\n",
            "          7.9245e-01, 5.5342e-01, 9.1486e-01],\n",
            "         [1.7313e-01, 2.7579e-01, 8.9621e-01, 4.9660e-01, 5.9359e-01,\n",
            "          5.7937e-01, 7.5915e-01, 9.3905e-01, 7.2902e-01, 6.7287e-01,\n",
            "          9.7149e-01, 6.7957e-01, 1.7870e-01, 9.6468e-01, 4.3967e-01,\n",
            "          6.3640e-01, 5.9696e-01, 7.4814e-01, 3.0175e-01, 6.3713e-01,\n",
            "          1.4997e-01, 3.2781e-01, 2.9215e-01, 6.1517e-01, 7.0013e-02,\n",
            "          2.5672e-01, 6.6226e-01, 8.1887e-02],\n",
            "         [9.4080e-01, 7.2637e-01, 1.1023e-01, 2.9393e-01, 7.0771e-01,\n",
            "          2.5200e-01, 1.4397e-01, 6.0791e-01, 8.1473e-01, 7.5223e-01,\n",
            "          3.9625e-01, 9.9612e-01, 6.9479e-01, 6.0413e-01, 6.0002e-02,\n",
            "          6.9136e-02, 6.3637e-01, 8.9954e-01, 9.9050e-01, 4.6357e-01,\n",
            "          9.2418e-01, 3.7569e-02, 9.3696e-01, 4.2771e-01, 4.4835e-02,\n",
            "          4.6242e-01, 2.1979e-01, 1.4299e-01],\n",
            "         [1.0437e-01, 6.9668e-01, 4.2823e-02, 7.5112e-02, 3.9171e-02,\n",
            "          9.2884e-01, 9.0606e-01, 2.6414e-01, 9.0774e-01, 2.4156e-01,\n",
            "          8.9913e-01, 3.4733e-01, 1.9416e-01, 5.3252e-01, 8.9346e-01,\n",
            "          5.4586e-01, 7.6373e-01, 1.0671e-01, 6.4727e-01, 5.6315e-01,\n",
            "          3.2953e-01, 1.8033e-01, 9.6148e-01, 1.6592e-01, 8.5814e-01,\n",
            "          8.7628e-01, 5.8449e-01, 5.8529e-01],\n",
            "         [1.8640e-01, 8.8532e-02, 3.0584e-01, 2.0289e-01, 3.1092e-01,\n",
            "          2.6715e-01, 2.2303e-01, 4.2696e-01, 4.1146e-01, 1.2135e-01,\n",
            "          8.8883e-01, 6.7858e-01, 5.2115e-01, 2.0844e-01, 5.6126e-01,\n",
            "          4.3487e-01, 1.7184e-01, 3.6069e-01, 8.6843e-02, 5.7321e-01,\n",
            "          6.0828e-01, 8.0711e-01, 2.0275e-01, 9.6739e-01, 7.1678e-01,\n",
            "          6.3935e-02, 3.9602e-01, 5.3426e-01],\n",
            "         [7.9168e-01, 7.6208e-01, 9.8080e-01, 2.1511e-01, 9.8005e-01,\n",
            "          6.6452e-01, 6.6273e-01, 1.9741e-01, 4.8347e-01, 4.2289e-02,\n",
            "          5.7097e-01, 6.1105e-01, 5.4795e-01, 5.0221e-01, 6.7419e-01,\n",
            "          2.2377e-01, 6.3197e-01, 4.0745e-01, 1.9390e-01, 8.1934e-01,\n",
            "          1.0760e-01, 2.7175e-01, 3.4247e-01, 4.5551e-01, 1.9764e-01,\n",
            "          5.0459e-01, 1.5184e-01, 3.2789e-01],\n",
            "         [1.2909e-01, 3.6031e-01, 8.2003e-01, 5.2056e-01, 8.6876e-01,\n",
            "          7.7728e-01, 2.6492e-01, 4.1430e-01, 6.5494e-01, 9.1882e-01,\n",
            "          2.0421e-01, 4.2283e-01, 5.3476e-01, 1.8577e-01, 8.2717e-01,\n",
            "          5.2514e-01, 5.0757e-01, 8.2919e-01, 8.0178e-01, 3.4753e-01,\n",
            "          3.4222e-01, 9.8672e-01, 8.9111e-01, 4.0614e-01, 5.2444e-01,\n",
            "          1.0531e-01, 6.7109e-01, 8.6536e-02],\n",
            "         [4.8946e-02, 9.6199e-01, 6.6069e-01, 2.3642e-01, 5.7135e-01,\n",
            "          9.6191e-01, 6.3872e-01, 5.1813e-01, 4.6658e-01, 8.7351e-01,\n",
            "          9.8517e-01, 4.1076e-01, 1.4287e-01, 6.6999e-01, 2.5365e-01,\n",
            "          8.3620e-01, 5.0744e-01, 1.1972e-01, 2.6340e-01, 3.5494e-01,\n",
            "          9.0663e-01, 1.6699e-01, 7.9478e-01, 5.9290e-01, 3.8112e-01,\n",
            "          4.3739e-01, 4.0914e-01, 4.0817e-01],\n",
            "         [1.2643e-01, 4.7399e-01, 2.9959e-01, 1.5885e-01, 5.6669e-03,\n",
            "          3.2653e-01, 2.2670e-01, 3.6003e-01, 2.7086e-01, 6.4306e-01,\n",
            "          9.1252e-01, 3.4128e-01, 1.0970e-01, 8.6084e-01, 1.3273e-01,\n",
            "          2.5598e-02, 6.5797e-01, 1.1053e-02, 9.9420e-01, 2.8604e-01,\n",
            "          5.7531e-01, 6.2016e-01, 1.0441e-02, 9.9853e-01, 8.3664e-01,\n",
            "          8.0651e-01, 9.5306e-01, 8.4418e-02],\n",
            "         [9.2216e-01, 5.0693e-01, 2.4679e-01, 2.8459e-01, 8.3529e-01,\n",
            "          4.2528e-01, 7.5503e-01, 1.5160e-01, 9.2939e-01, 8.0012e-01,\n",
            "          1.5318e-01, 5.1605e-03, 5.5109e-01, 2.3090e-01, 2.8730e-01,\n",
            "          2.8117e-01, 6.0955e-01, 7.6258e-01, 9.5025e-01, 7.8953e-01,\n",
            "          4.2898e-01, 8.7269e-01, 3.3635e-01, 1.0644e-01, 8.9174e-01,\n",
            "          9.0172e-01, 9.0904e-01, 6.9514e-01],\n",
            "         [7.2583e-01, 3.3440e-01, 5.8907e-01, 2.1349e-01, 4.2581e-02,\n",
            "          4.0409e-01, 1.7354e-01, 6.4318e-01, 7.1771e-01, 1.8555e-01,\n",
            "          9.0838e-01, 4.7234e-01, 2.6621e-01, 1.5301e-01, 9.2230e-01,\n",
            "          1.1758e-01, 9.3116e-01, 2.0830e-01, 5.8894e-02, 5.0171e-01,\n",
            "          2.0486e-01, 1.2191e-01, 7.4956e-01, 3.2780e-01, 4.3654e-01,\n",
            "          3.5690e-01, 2.1449e-01, 1.8399e-01],\n",
            "         [4.4425e-01, 8.0905e-01, 3.9017e-01, 7.5178e-01, 8.7181e-01,\n",
            "          6.0919e-01, 4.8128e-01, 7.4233e-01, 2.3281e-01, 2.5035e-01,\n",
            "          7.8768e-02, 8.3779e-01, 2.2651e-01, 3.4840e-01, 7.8734e-01,\n",
            "          9.9441e-01, 8.1747e-01, 3.7535e-01, 9.8673e-01, 7.5631e-01,\n",
            "          6.8995e-01, 3.7518e-01, 3.6938e-01, 1.5978e-01, 6.5766e-01,\n",
            "          2.7569e-01, 4.5252e-01, 2.0269e-01]]], device='cuda:0')\n",
            "Predicted class: tensor([2], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eg7NBbTphDE"
      },
      "source": [
        "#モデルレイヤー\n",
        "サイズ28*28の３枚の画像からなるミニバッチのサンプルを用意する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSAo5vQXo6d3",
        "outputId": "f53b94dc-b1f2-4b9b-fd5c-871f55fc2709"
      },
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmvIS_ZJqQRS"
      },
      "source": [
        "nn.Flatten\n",
        "nn.Flattenレイヤーで二次元(28*28)の画像を、1次元の784ピクセルの値へと変換する\n",
        "ミニバッチの0次元目はサンプル番号を示す次元でこの次元はnn.Flattenを通しても変化しない"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoArZwzGqH_d",
        "outputId": "61e6cbfe-8ad2-4fc0-8eed-ad955a13ae81"
      },
      "source": [
        "#nn.flattenレイヤー\n",
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(input_image.size())\n",
        "print(flat_image.size())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n",
            "torch.Size([3, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyZgaWqnrtLi"
      },
      "source": [
        "#nn.Linear\n",
        "linear_layerは線形変換を施します＝＞inputで前の層のサイズを受け取ってoutputで変換したいラベル数に変換\n",
        "linear layerは重みとバイアスのパラメータを保持している"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvlqCqAErGD4",
        "outputId": "24eaf636-8284-4965-85c5-c94d3c256da2"
      },
      "source": [
        "layer1 = nn.Linear(in_features=784,out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4nsFYJf6JT6"
      },
      "source": [
        "nn.ReLu\n",
        "非線形な活性化関数はニューラルネットワークの入力と出力の間にある複雑な関係性を表現するのに重要な要素\n",
        "これらは活性化関数の線形変換の後に非線形性を加えニューラルネットワークの表現力を向上させる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u63hQjWYs9ik",
        "outputId": "772fc429-8fe9-4ea0-aecc-f55885baba86"
      },
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLU: tensor([[ 0.1080,  0.4248, -0.0817, -0.1225,  0.2427, -0.0938, -0.3603,  0.1902,\n",
            "         -0.2964, -0.1737,  0.6111, -0.3730, -0.2087,  0.2685,  0.0957,  0.1420,\n",
            "          0.3176,  0.1956,  0.2258,  0.7737],\n",
            "        [-0.4898,  0.6161,  0.2033,  0.4319,  0.3017, -0.1329,  0.0991, -0.0050,\n",
            "         -0.5110, -0.5223,  0.2197,  0.0526, -0.1396, -0.0138,  0.0627,  0.1823,\n",
            "          0.3204,  0.2687,  0.0041,  0.4765],\n",
            "        [-0.1007,  0.2797,  0.0823,  0.3257,  0.0645, -0.0568,  0.1974,  0.3998,\n",
            "         -0.2636, -0.3114,  0.2634, -0.3282,  0.0595, -0.0317, -0.0544,  0.1262,\n",
            "          0.3395,  0.1179,  0.0358,  0.8704]], grad_fn=<AddmmBackward>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.1080, 0.4248, 0.0000, 0.0000, 0.2427, 0.0000, 0.0000, 0.1902, 0.0000,\n",
            "         0.0000, 0.6111, 0.0000, 0.0000, 0.2685, 0.0957, 0.1420, 0.3176, 0.1956,\n",
            "         0.2258, 0.7737],\n",
            "        [0.0000, 0.6161, 0.2033, 0.4319, 0.3017, 0.0000, 0.0991, 0.0000, 0.0000,\n",
            "         0.0000, 0.2197, 0.0526, 0.0000, 0.0000, 0.0627, 0.1823, 0.3204, 0.2687,\n",
            "         0.0041, 0.4765],\n",
            "        [0.0000, 0.2797, 0.0823, 0.3257, 0.0645, 0.0000, 0.1974, 0.3998, 0.0000,\n",
            "         0.0000, 0.2634, 0.0000, 0.0595, 0.0000, 0.0000, 0.1262, 0.3395, 0.1179,\n",
            "         0.0358, 0.8704]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPF25lq67CBk"
      },
      "source": [
        "nn.Sequential\n",
        "モジュールを順番に格納する箱のようなもの\n",
        "入力データはSequentialに定義された順番に各モジュールを伝搬する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5repbUoj6mxo"
      },
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20,10)\n",
        ")\n",
        "input_image = torch.rand(2,28,28)\n",
        "logits = seq_modules(input_image)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljgk2XuQ7qGZ"
      },
      "source": [
        "nn.softmax\n",
        "ニューラルネットワークの最後のlinear layerはlogits[-∞,∞]を出力する\n",
        "このlogitsはnn.Softmaxモジュールへと渡される\n",
        "その結果、最終的な値は[0~1]の範囲となり、各クラスの確率を表現する\n",
        "dimパラメータは次元を示していて、dim=1の次元で和を求めると確率の総和なので1になる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYTakjny7hAA"
      },
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoX-fCgl8boo"
      },
      "source": [
        "ニューラルネットワークを構成する多くのモジュールは各々パラメータを保持している\n",
        "例えば重みやバイアスであり、これらが訓練時に最適化される\n",
        "\n",
        "nn.Moduleを継承することで、モデルオブジェクト内で定義された全てのフィールドが自動的に追跡できるようになりparameters()やnamed_parameters()メソッドを使って、モデルの各レイヤーの全てのパラメータにアクセスできるようになる\n",
        "以下ではfor文を用いて各パラメータを処理し、そのサイズと値を表示している"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ6_xRSg8Vzp",
        "outputId": "af46b704-cd1c-4410-a17a-04e5ad64547e"
      },
      "source": [
        "print(model)\n",
        "\n",
        "for name,param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0299,  0.0289, -0.0056,  ..., -0.0285, -0.0082, -0.0242],\n",
            "        [ 0.0232,  0.0313,  0.0202,  ...,  0.0195, -0.0102, -0.0112]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0238, -0.0168], device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0007,  0.0238,  0.0376,  ..., -0.0394, -0.0129, -0.0384],\n",
            "        [ 0.0172,  0.0153,  0.0250,  ...,  0.0421,  0.0052,  0.0032]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0160, 0.0213], device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0414, -0.0198, -0.0122,  ..., -0.0168, -0.0355,  0.0200],\n",
            "        [ 0.0351, -0.0120,  0.0187,  ...,  0.0316, -0.0437,  0.0298]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0054, -0.0205], device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovwSXRUC_Kxi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}